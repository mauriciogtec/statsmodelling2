---
title: 'Statistical Modelling II'
author: 'Mauricio Garcia Tec'
output: 
  html_notebook: 
    toc: yes
---

$$
\DeclareMathOperator{\N}{N}
\DeclareMathOperator{\Gam}{Gamma}
\DeclareMathOperator{\Wish}{Wishart}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\Po}{Poisson}
\DeclareMathOperator{\Prob}{P}
$$

[Return to main Index](https://github.com/mauriciogtec/statsmodelling2)

```{r setup, include=FALSE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE)
```


# Section 5: Mixture models

## Mixture Models

So far, we've assumed that our data are conditionally exchangeable given their covariates. In other words, for every unique set of covariates there exists a set of parameters, conditioned on which, the data with those covariates are i.i.d. We used various distributions over functions to learn a distribution over these parameters, for all covariate settings.

A common setting was when our data was normally distributed, with mean $\beta^Tx_i$ and variance $\sigma^2$. If we did not have the covariate values $x_i$, our data would no longer be normally distributed.

### Exercise 1

*Download the dataset restaurants.csv. This contains profit information for restaurants, based on seating capacity and whether they are open for dinner. Run a Bayesian regression of Profit vs SeatingCapacity and a dummy for DinnerService (you can reuse code from 2.12) (I'd suggest whitening Profit, it will make later prior specification easier). Do the residuals look normal? (e.g.\ plot histograms, qq plots). Now, let's just look at the raw Profit data: Does it look normal?*


*Solution*. First we read the data.

```{r}
library(tidyverse)
library(rstan)
```

```{r}
resto <- read_csv("./restaurants.csv")[ ,-1]
head(resto)
```

```{r}
dim(resto)
```

We'll use stan for our Bayesian linear regression.

```{stan, eval=FALSE}
// Stan model 1 for problem 1
data {
  // meta
  int<lower=1> N; // nobs
  // data
  vector[N] x; // feature
  vector[N] y; // response
  // prior
  real<lower=0> a0; // omega ~ Gamma(a0, shape = b0) !
  real<lower=0> b0;
  real mu0; // beta ~ N(mu0, (kappa0*omega)^{-1})
  real<lower=0> kappa0; 
}
parameters {
  real alpha; // intercept
  real beta; // lin coefs
  real<lower=0> omega; // precision
}
transformed parameters {
  real<lower=0> sigma; // standard dev
  vector[N] yhat; // linpred
  sigma = pow(omega, -0.5);
  yhat = alpha + x * beta;
}
model {
  // prior
  // alpha ~ 1; it's assumed
  omega ~ gamma(a0, b0);
  beta ~ normal(mu0, sigma / kappa0);
  // likelihood
  y ~ normal(yhat, sigma);
}
generated quantities {
  vector[N] residuals;
  residuals = y - yhat;
}
```

```{r}
stan1 <- stan_model(file = "./stan1.stan")
```


```{r}
resto <- resto %>% 
  mutate(SeatingCapacity = as.numeric(SeatingCapacity)) %>% 
  mutate(profit_scaled = as.numeric(scale(Profit, scale = TRUE, center = FALSE)))
x <- resto %>% 
  pull(SeatingCapacity) 
y <- resto %>% 
  pull(profit_scaled)
```

```{r}
standat <- list(
  N = nrow(resto),
  x = x,
  y = y,
  a0 = 1.,
  b0 = 0.01,
  mu0 = 0.,
  kappa0 = 1.)
```

```{r}
fit1 <- sampling(
  stan1, 
  data = standat,
  iter = 500,
  chains = 2,
  cores = 2)
```

```{r}
summary(fit1)$summary %>% 
  as.data.frame() %>% 
  head(4)
```

```{r}
residuals <- rstan::extract(fit1, "residuals")$residuals
```

```{r}
all_residuals <- as.numeric(residuals)
hist(all_residuals, breaks = 50)
```

That's it! We see a bimodal shape on the residuals. Let's see more evidence.

```{r}
qqnorm(all_residuals, col = "gray", border = "white")
```

Again, not normal! 

[Back to Index](#TOC)

----

Let's assume we're in the situation where we don't know any of these covariate values. For now, let's ignore the continuous-valued covariate (SeatingCapacity), and try to infer the categorical covariate. Let's say we know that half our restaurants are open for dinner. We could assume that each restaurant is associated with a \textit{latent} indicator variable $Z_i$, that assigns them to one of two groups, so that

$$Z_i \sim \mbox{Bernoulli}(\pi)$$

As in the regression setting, conditioned on the latent variable, we will assume that the observed profits are i.i.d.\ normal. Again, as in the basic regression setting, we will assume the variances of the two normals are the same, but the means are different, i.e.

$$X_i|Z_i=z \sim \mbox{Normal}(\mu_{z}, \sigma^2).$$

If we marginalize over these binary indicators, our observations are assumed to be distributed according to a mixture of two Gaussians:

$$X_i \sim 0.5N(\mu_1,\sigma_1^2) + 0.5(\mu_2,\sigma_2^2)$$

We can then look at the posterior distribution over each indicator variable, conditioned on the class probabilities and parameters:

$$\begin{aligned}\Prob(Z_i = z|X_i, \pi, \mu_1,\sigma^2) \propto & \Prob(Z_i=z|\pi)p(X_i|\mu_z,\sigma^2)\\
  \mbox{so, }\qquad \Prob(Z_i=1|X_i, \pi, \mu_1,\sigma^2) \propto & \pi p(X_i|\mu_1,\sigma^2)\\
  \Prob(Z_i=0|X_i, \pi, \mu_1,\sigma^2) \propto & \Prob(Z_i=0|\pi)p(X_0|\mu_z,\sigma^2)
\end{aligned}$$

Conditioned on the $Z_i$, we can update the means of the Gaussians using conjugacy.

Note that we are not guaranteed to find latent clusters that correspond to the covariate we were expecting! If there is a more parsimonious partitioning of the data, then the posterior will tend to favor that partitioning.

### Exercise 2

*Let's assume (as is the case if our latent variables correspond to the actual DinnerService covariate) that the class proportions are roughly equal, and fix $\pi=0.5$. Using the conditional distributions $P(Z_i|X_i,\pi,\mu_1,\mu_2,\sigma^2)$ and $p(\mu_k|\{X_i:Z_i=k\}, \theta)$, where $\theta$ are appropriate (shared) prior parameters for $\mu_k$, implement a Gibbs sampler that samples the means and the latent indicator variables. I'd suggest using the parameters of the initial regression to pick your hyperparameters. Compare the clustering obtained with the ``true'' clustering due to the DinnerService variable.*

*Solution*. In this problem we want to estimate the density of Profits, which shows a slightly multimodal behaviour.

```{r}
hist(y, breaks = 40, col = "gray", border = "white")
```


The Gibbs sampler is very easy in this case because the weights are fixed. We only need to add the sampling step.

```{r}
gibbs2 <- function(nsim, y, prior) {
  # meta
  n <- length(y)
  
  # parameters
  mu <- matrix(0, nsim + 1, 2)
  omega <- numeric(nsim + 1)
  sigma <- numeric(nsim + 1)
  latent <- matrix(0, nsim + 1, n)
  
  # initialise
  omega[1] <- prior$a0 / prior$b0 # beta is rate here!
  sigma[1] <- 1 / sqrt(omega[1]) # std.dev
  mu[1, ] <- prior$mu0 # sensible values for mu
  for (i in 1:n) {
    wts <- 0.5 * map_dbl(mu[1, ], ~ dnorm(y[i], ., sigma[1]))
    latent[1, i] <- sample(2, 1, prob = wts / sum(wts)) # allocate ever individual
  }
  
  # constant quantities
  an <- prior$a0 + n/2 + 1 / 2
  
  # sampler
  for (k in 1:nsim) {
    # counts and means per component
    nj <- map_int(1:2, ~ sum(latent[k, ] == .))
    meanj <- map_dbl(1:2, ~ ifelse(nj[.] > 0, mean(y[latent[k, ] == .]), 0))
    # posterior parameters
    kappanj <- nj  + prior$kappa0 
    munj <- (nj * meanj + prior$kappa0 * prior$mu0) / kappanj
    bn <- prior$b0 + 0.5*sum((y - meanj[latent[k, ]])^2) +
      0.5*sum(prior$kappa0 * nj / kappanj * (meanj - prior$mu0)^2)
  
    # sample gamma  
    omega[k + 1] <- rgamma(1, an, bn)
    sigma[k + 1] <-  1 / sqrt(omega[k + 1])
    
    # sample the means
    mu[k + 1, ] <-  map_dbl(1:2, ~
      rnorm(1, munj[.], sigma[k + 1] / sqrt(kappanj[.])))
    # allocate again
    for (i in 1:n) {
      wts <- 0.5 * map_dbl(mu[k + 1, ], ~ dnorm(y[i], ., sigma[k + 1]))
      latent[k + 1, i] <- sample(2, 1, prob = wts / sum(wts))
    }
  }
  
  list(mu = mu[-1, ], omega = omega[-1], 
       sigma = sigma[-1], latent = latent[-1, ])
}
```

```{r}
nsim <- 100
prior <- list(mu0 = c(0.3, 1.2), a0 = 1, b0 = .01, kappa0 = 1.)
sim2 <- gibbs2(nsim, y, prior)
```

```{r}
{
grid_length <- 100
gridy <- seq(0.4, 1.6, length.out = grid_length)
hist(y[resto$DinnerService == 0], breaks= 20, prob = TRUE, 
  col = alpha("blue", 0.1), border = "white", xlim = c(0.3, 1.7), xlab = "Price",
  main = "Mixture density estimation")
hist(y[resto$DinnerService == 1], breaks= 20, prob = TRUE, add = TRUE,
  col = alpha("red", 0.1), border = "white")
hist(y, breaks= 40, prob = TRUE, add = TRUE,
  col = alpha("black", 0.1))
mean_dens <- numeric(grid_length)
for (k in 1:nsim) {
  dens <- 0.5 * dnorm(gridy, sim2$mu[k, 1], sim2$sigma[k]) +
    0.5 * dnorm(gridy, sim2$mu[k, 2], sim2$sigma[k])
  mean_dens <- mean_dens + dens / nsim
  lines(gridy, dens, col = alpha("black", 0.03), lw = 3)
}
lines(gridy, mean_dens, col = alpha("black", 0.8), lw = 2)
lines(density(y), col = "magenta")
legend("topright", c("price[service=0]", "price[service=1]", "price", "posterior density", "kde estimate"),
  pch = c(15, 15, 15, NA, NA), lt = c(NA, NA, NA, 1, 1, 1),
  col = c(alpha("blue", 0.4), alpha("red", 0.4), alpha("gray", 0.5), "black", "magenta"))
}
```

---


OK, let's now assume we don't know $\pi$, and that the two classes have different values of $\sigma^2$. Let's put a Beta$(\alpha,\beta)$ prior on $\pi$, since it is conjugate to the Bernoulli distribution. 


### Exercise 3

*Let's assume we want to integrate out $\pi$. What is the conditional distribution $P(z_i|z_{\neg i}, X_i,\mu_1,\mu_2,\sigma_1,\sigma_2,\alpha,\beta)$, where $Z_{\neg i}$ means all the values of $z$ except $z_i$?*

*Solution*. Write $\theta = (\alpha,\beta)$, $\mu=(\mu_1,\mu_2)$. Assume aprior
$$
\pi \sim \mathrm{Beta}(\nu_1,\nu_2)
$$
First we note that
$$
p(z_i\mid z_{\neg i}, x, \mu,\sigma^2,\theta) \propto N(x_i \mid \mu_{z_i},\sigma^2)p(z_i\mid z_{\neg i},\mu,\sigma^2,\theta)
$$
For the second term will do the following
$$
\begin{aligned}
p(z_i = 1\mid z_{\neg i},\mu,\sigma^2,\theta) & = 
\int p(Z_i = 1\mid \pi, z_{\neg i},\mu,\sigma^2,\theta)p(\pi\mid z_{\neg i},\mu, \sigma^2, \theta) d\pi \\
& = \int \pi p(\pi\mid z_{\neg i},\mu, \sigma^2, \theta) d\pi \\
& \propto \int \pi p(z_{\neg i} \mid \pi ,\mu, \sigma^2, \theta) p(\pi)d\pi \\
& \propto \int \pi \mathrm{Beta}(\pi \mid n_{1,\neg i} + \nu_1, n_{2,\neg i} +\nu_2)d\pi \\
& \propto E \left[ \mathrm{Beta}(n_{1,\neg i} + \nu_1, n_{2,\neg i} +\nu_2) \right] \\
& = \frac{n_{1,\neg i} + \nu_1}{n - 1+ \nu_1 + \nu_2}.
\end{aligned}
$$
where $n_{k,\neg i} = \sum_{j \neq i}1(Z_j=k)$ for $k=1,2$. Note that $n = n_{1,\neg i} + n_{2,\neg i} + 1$ is the size  $X$. Similarly,
$$
p(z_i = 2\mid z_{\neg i},\mu,\sigma^2,\theta) = \frac{n_{2,\neg i} + \nu_2}{n - 1+ \nu_1 + \nu_2}.
$$
We can write this succintly as
$$
p(z_i \mid z_{\neg i},\mu,\sigma^2,\theta) = \frac{n_{z_i,\neg i} + \nu_{z_i}}{n - 1+ \nu_1 + \nu_2}.
$$
So as a conclusion we have
$$
p(z_i = k\mid z_{\neg i},x_i, \mu,\sigma^2,\theta) \propto N(x_i \mid \mu_{k},\sigma^2)\frac{n_{k,\neg i} + \nu_{k}}{n - 1+ \nu_1 + \nu_2} \quad \text{ for } \quad k=1,2
$$

[Back to Index](#TOC)

----


### Exercise 4

*How about if we want to integrate out all of the continuous variables? What is the conditional distribution $P(Z_i|Z_{\neg i}, X, \theta)$, where $\theta$ is the set of all hyperparameters?*

*Solution*. So far I have been working with a single parameter $\sigma$ for all clusters. The math will be a little simpler if we allow a different $\sigma_j$ for each cluster. It's not that harder in the other case, but it's also an opportunity for extending the model. To simplify denote the parameters as
$$
\eta = (\eta_1, ..., \eta_K) = ((\mu_1,\sigma^2_1), ...., (\mu_K,\sigma_K^2))
$$
with conugate independent priors
$$
\mu_j \sim N(\mu_{0,j}, \sigma_j^2 / \kappa_{0}) \quad \sigma_j^2 \sim \text{InvGamma}(a_0, b_0),
$$
 For the mixture model, there are weights $w_j$ and latent variables $z_i$ 
such that
$$
p(x_i, z_i = j \mid w, \eta, \theta) = w_j N(x_i \mid \eta_j).
$$
with priors
$$
p(z_i = j \mid \pi) = w_j \quad \text{and} \quad \pi \sim \mathrm{Dirichlet}\left(\nu_1, ..., \nu_K\right).
$$
The hyperparameters are $\theta = (\kappa_0, a_0, b_0, \mu_0, \nu)$. For notational hygiene, I'll treat them as constants and not include them in the probability notations.

Finally, as a convenient notation, denote 
$$
n^{\neg i}_j:=\sum_{l=1, l \neq i}^n 1(z_l = j)
$$ 
as the total counf of latent allocation to cluster $j$ excluding observation $i$. Evidently, $\sum_j  n^{\neg i}_j = n - 1$ where $n$ is the total number of observations. In a similar spirit, we'll denote 
$$
\bar{x}^{\neg i}_{j}:= \frac{1}{n^{\neg i}_j} \sum_{l=1, l \neq i}^n x_i 1(z_l = j)
$$ 
the mean of the observed data for all the points excluding $x_i$.

Recall that as a result from the previous exercise we have that
$$
p(z_i = j \mid z_{\neg i},\eta) = \frac{\nu_j + n^{\neg i}_j}{\sum_k(\nu_k + n^{\neg i}_k)}.
$$
and therefore
$$
p(z_i = j \mid x_i, z_{\neg i},\eta) \propto N(x_i \mid \eta_j) \frac{\nu_j + n^{\neg i}_j}{\sum_k(\nu_k + n^{\neg i}_k)}.
$$

We now derive the expression required for the collapsed Gibbs sampler. Be mindful that $x$ denote the entire data, $x_i$ the i-th observation, and $x_{\neg i}$ the data removing the i-th observation. Also, we are being careful not to use the proportionality sign if the constant can't be taken outside the integral.

An application of the lat of total probability followed by Bayes' theorem on the second term inside the integral (separating $x_i$ from $x$) yields
$$
\begin{aligned}
p(z_i  \mid z_{\neg i}, x) &= \int p(z_i \mid z_{\neg i}, x, \eta) p(\eta \mid z_{\neg i}, x) d\eta \\
& \propto \int p(z_i \mid z_{\neg i},x , \eta) p(x_i \mid z_i, x_{\neg i}, \eta)p(\eta\mid z_{\neg i}, x_{\neg i}) d\eta.
\end{aligned}
$$
We now have the posterior for all the data excluding observation $i$ appearing as the right-term of the integrand. For the other two term we observe the identity (another application of Bayes' theorem separating $x_i$ from $x$ on the left-term)
$$
\begin{aligned}
p(z_i \mid z_{\neg i}, x , \eta) p(x_i \mid \eta, z_i, x_{\neg i}) & = \frac{p(x_i \mid z, x_{\neg i}, \eta)p(z_i \mid z_{\neg i}, x_{\neg i}, \eta)}{p(x_i \mid z_i, x_{\neg i}, \eta)}p(x_i \mid z_i, x_{\neg i}, \eta) \\
& = p(x_i \mid z, x_{\neg i}, \eta)p(z_i \mid z_{\neg i}, x_{\neg i}, \eta) \\
& = p(x_i \mid z_i, \eta)p(z_i \mid z_{\neg i}, \eta) \\
& = N(x_i \mid \eta_{z_i})\frac{\nu_j + n^{\neg i}_j}{\sum_k(\nu_k + n^{\neg i}_k)}
\end{aligned}
$$
where the third equality holds because the $x_i$ are independent of the $x_{\neg i}$ and $z_{\neg i}$ given $\eta$; and similarly, $z_i$ is independent of $x_{\neg i}$ given $z_{\neg i}$ and $\eta$. Now plugging this back into the integral we get
$$
p(z_i  \mid z_{\neg i}, x) \propto (\nu_j + n^{\neg i}_j)\int N(x_i \mid \eta_{z_i}) p(\eta \mid z_{\neg i}, x_{\neg i}) d\eta
$$
Finally, our particular choice of priors makes the posterior of $\eta$ to split as 
$$
 p(\eta \mid z_{\neg i}, x_{\neg i}) = \prod_{j=1}^Kp(\eta_j \mid z_{\neg i}, x_{\neg i}).
$$
So we have
$$
(\nu_j + n^{\neg i}_j)\int N(x_i \mid \eta_{z_i}) p(\eta \mid z_{\neg i}, x_{\neg i}) d\eta = (\nu_j + n^{\neg i}_j)\int N(x_i \mid \eta_{z_i}) p(\eta_{z_i} \mid z_{\neg i}, x_{\neg i}) d\eta_{z_i},
$$
and this integral is a piece of cake, since we have computed this integral many times: it is the posterior predictive of a normal distribution, which we know it's a t-distribution.

So putting all together we arrive to
$$
p(z_i  \mid z_{\neg i}, x) \propto (\nu_j + n^{\neg i}_j) \,\mathrm{tStudent}(x_i \mid \mu_{n,z_i}, b_j^{\neg i} / (\kappa_0 a^{\neg i}_j), 2a^{\neg i}_j )
$$

* $\mu_n = \frac{n_{j, -i}}{n_{j, -i} + \kappa_0}\bar{x}_{j, -i} + \frac{\kappa_0}{n_{j, -i} + \kappa_0}\mu_{0,j}$
* $a_n = n_{j, -i} - (n - 1 - 1) / 2 + K / 2$
* $b_n = \frac{1}{2}\sum_{z_k = j, k\neq i}(x_i - \bar{x}_{j, -i} )^2 + \frac{n_{j,-1}\kappa_0 }{n_{j,-1} + \kappa_0}(\bar{x}_{j, -1} - \mu_{0,j})^2$


[Back to Index](#TOC)

----

### Exercise 5

*Implement a Gibbs sampler for this new model where we learn the cluster proportions. You can either implement one of the variants in the previous two exercises, or the fully uncollapsed model where we sample $Z$, $\pi$, $\mu_1$, $\mu_2$, $\sigma^2_1$ and $\sigma^2_2$.*

*Solution*. I'll implement the sampler from Exercise 3 since it's just an adaptation from Exercise 2. All we need to do is to change the resampling scheme of the $z$ at the end of the inner loop according to the expression found at Exercise 3.

```{r}
gibbs5 <- function(nsim, y, prior) {
  # meta
  n <- length(y)
  
  # parameters
  mu <- matrix(0, nsim + 1, 2)
  omega <- numeric(nsim + 1)
  sigma <- numeric(nsim + 1)
  latent <- matrix(0, nsim + 1, n)
  # transformed parameter
  weights <- matrix(0, nsim + 1, 2)
  
  # initialise
  omega[1] <- prior$a0 / prior$b0 # beta is rate here!
  sigma[1] <- 1 / sqrt(omega[1]) # std.dev
  mu[1, ] <- prior$mu0 # sensible values for mu
  for (i in 1:n) { # smart starting value according to prior
    latent[1, i] <- sample(2, 1, prob = prior$nu / sum(prior$nu)) # allocate ever individual
  }
  nj <- map_int(1:2, ~ sum(latent[1, ] == .))
  weights[1, ] <- prior$nu / sum(prior$nu)
  
  # constant quantities
  an <- prior$a0 + n/2 + 1 / 2
  
  # sampler
  for (k in 1:nsim) {
    # counts and means per component
    meanj <- map_dbl(1:2, ~ ifelse(nj[.] > 0, mean(y[latent[k, ] == .]), 0))
    # posterior parameters
    kappanj <- nj  + prior$kappa0 
    munj <- (nj * meanj + prior$kappa0 * prior$mu0) / kappanj
    bn <- prior$b0 + 0.5*sum((y - meanj[latent[k, ]])^2) +
      0.5*sum(prior$kappa0 * nj / kappanj * (meanj - prior$mu0)^2)
  
    # sample gamma  
    omega[k + 1] <- rgamma(1, an, bn)
    sigma[k + 1] <-  1 / sqrt(omega[k + 1])
    
    # sample the means
    mu[k + 1, ] <-  map_dbl(1:2, ~
      rnorm(1, munj[.], sigma[k + 1] / sqrt(kappanj[.])))
    
    # allocate again
    latent[k + 1, i] <- latent[k, i] # make a copy
    for (i in 1:n) { 
      cnts <- nj + prior$nu - as.integer(latent[k + 1, i] == 1:2)
      wts <- cnts * map_dbl(mu[k + 1, ], ~ dnorm(y[i], ., sigma[k + 1]))
      latent[k+ 1, i] <- sample(2, 1, prob = wts / sum(wts)) # allocate ever individual
    }
    nj <- map_int(1:2, ~ sum(latent[k + 1, ] == .))
    weights[k + 1, ] <- (nj + prior$nu) / sum(nj + prior$nu)
  }
  
  list(mu = mu[-1, ], omega = omega[-1], 
       sigma = sigma[-1], latent = latent[-1, ], weights = weights[-1, ])
}
```

```{r}
nsim <- 500
prior <- list(mu0 = c(0.3, 1.2), a0 = 1, b0 = .01, kappa0 = 1., nu = c(1, 1))
sim5 <- gibbs5(nsim, y, prior)
```

We can graph the result as in Exercise 2 to see the results.

```{r}
{
warmup <- 250
grid_length <- 100
gridy <- seq(0.4, 1.6, length.out = grid_length)
hist(y[resto$DinnerService == 0], breaks= 20, prob = TRUE, 
  col = alpha("blue", 0.1), border = "white", xlim = c(0.3, 1.7), xlab = "Price",
  main = "Problem 5: Normal Mixture with Collapsed Sampler")
hist(y[resto$DinnerService == 1], breaks= 20, prob = TRUE, add = TRUE,
  col = alpha("red", 0.1), border = "white")
hist(y, breaks= 40, prob = TRUE, add = TRUE,
  col = alpha("black", 0.1))
mean_dens <- numeric(grid_length)
for (k in (warmup + 1):nsim) {
  dens <- sim5$weights[k, 1] * dnorm(gridy, sim5$mu[k, 1], sim5$sigma[k]) +
   sim5$weights[k, 2] * dnorm(gridy, sim5$mu[k, 2], sim5$sigma[k])
  mean_dens <- mean_dens + dens / (nsim - warmup)
  lines(gridy, dens, col = alpha("black", 0.02), lw = 3)
}
lines(gridy, mean_dens, col = alpha("black", 0.8), lw = 2)
lines(density(y), col = "magenta")
legend("topright", c("price[service=0]", "price[service=1]", "price", "posterior density", "kde estimate"),
  pch = c(15, 15, 15, NA, NA), lt = c(NA, NA, NA, 1, 1, 1),
  col = c(alpha("blue", 0.4), alpha("red", 0.4), alpha("gray", 0.5), "black", "magenta"))
}
```

[Back to Index](#TOC)


----


Let's now consider the case where we have more than two classes. Here, we need to replace our Bernoulli distribution with a multinomial parametrized by some probability vector $\pi$, so that:

$$P(Z_i = k) = \pi_k$$

  Much as the multinomial is the multivariate generalization of the binomial distribution, the Dirichlet($\alpha_1,\dots,\alpha_K$) distribution, which has pdf
  $$\frac{\Gamma(\sum_k \alpha_k)}{\prod_k \Gamma(\alpha_k)} \prod_{k=1}^K \pi_k^{\alpha_k},$$
  is the multivariate generalization of the beta distribution. Here, $\alpha$ is a $D$-dimensional vector where $\alpha_k>0$ and $\sum_k\alpha_k\geq 1$. The expectation of a Dirichlet distribution is given by the normalized parameter vector, $E[\pi] = \frac{(\alpha_1,\dots,\alpha_K)}{\sum_k\alpha_k}$. The absolute magnitude of the parameter acts like an inverse variance: the smaller its values, the further a given sample is from the expected value. Figure below shows the pdf of three Dirichlet distributions represented on the 3-simplex, with samples from those distributions
  
![](images/dirichlet.png)


### Exercise 6

*Show that the Dirichlet is conjugate to the multinomial, and derive the posterior predictive distribution
  $$P(Z_{n+1}|Z_{1:n}) = \int_{\mathcal{M}} P(Z_{n+1}|\pi)p(\pi \mid Z_{1:n}) d\pi$$
  You may find it helpful to note that, if $\pi\sim \mbox{Dirichlet}(\alpha_1,\dots,\alpha_K)$, then $E[\pi] = \frac{(\alpha_1,\dots,\alpha_K)}{\sum_k\alpha_k}$.*

*Solution*. The key is to write the distribution the multinomial distribution as
$$
\mathrm{Multinomial}(Z\mid \pi_1, ..., \pi_K) \propto \pi_1^{1(Z=1)} \cdots \pi_K^{1(Z = K)}
$$
Then
$$
\begin{aligned}
p(\pi \mid Z) &\propto p(\pi)\prod_ip(Z_i \mid \pi) \\
& \propto \pi_1^{\alpha_1 - 1} \cdots \pi_K^{\alpha_K - 1} \prod_i \pi_1^{1(Z_i=1)}\cdots \pi_K^{1(Z_i=K)} \\
& \propto \pi_1^{\alpha_1 -1 + N_1} \cdots \pi_K^{\alpha_K -1 + N_k} \\
& \propto \mathrm{Dirichlet}\left(\pi \mid \alpha_1 + N_1 -1, ...,  N_K + \alpha_K -1\right) 
\end{aligned}
$$
where $N_j = \sum_i 1(Z_i = j)$. This also helps us to interpret the initial hyperparameters $\alpha_j$ of the Dirichlet Distribution as pseudo-counts.

The key for the following is to observe the following simple identity
$$
P(Z \mid \pi) = \pi
$$
Then the posterior predictive is
$$
\begin{aligned}
P(Z_{n+1} \mid Z_{1:n}) & = \int_{\mathcal{M}}  P(Z_{n+1} \mid \pi)p(\pi \mid Z_{1:n}) d\pi \\
& \propto  \int_{\mathcal{M}}  \pi p(\pi \mid Z_{1:n})  d\pi \\
& =  E\left[\mathrm{Dirichlet}\left(\pi \mid \alpha_1 + N_1 -1, ...,  N_K + \alpha_K -1\right) \right] \\
& \propto  \mathrm{Multinomial}\left(\left(\frac{\alpha_j + N_j}{\sum_j(\alpha_j + N_j)}\right)_{j=1}^K\right). \\
\end{aligned}
$$


[Back to Index](#TOC)


----


### Exercise 7

*Modify your previous Gibbs sampler to allow multiple classes, and two-dimensional data. Generate some data according to a Dirichlet mixture of 5 Gaussians in $\mathbb{R}^2$, and test your code on it.*

Here's the modified Gibbs sampler. Tiny modifications and using a Dirichlet. As a nice bonus I'm adding a process bar.

```{r}
library(tidyverse)
library(progress)
```


```{r}
gibbs7 <- function(nsim, y, prior) {
  # meta
  n <- length(y)
  K <- length(prior$mu0)
  
  # parameters
  mu <- matrix(0, nsim + 1, K)
  omega <- numeric(nsim + 1)
  sigma <- numeric(nsim + 1)
  latent <- matrix(0, nsim + 1, n)
  # transformed parameter
  weights <- matrix(0, nsim + 1, K)
  
  # initialise
  omega[1] <- prior$a0 / prior$b0 # beta is rate here!
  sigma[1] <- 1 / sqrt(omega[1]) # std.dev
  mu[1, ] <- prior$mu0 # sensible values for mu
  for (i in 1:n) { # smart starting value according to prior
    latent[1, i] <- sample(K, 1, prob = prior$nu / sum(prior$nu)) # allocate ever individual
  }
  nj <- map_int(1:K, ~ sum(latent[1, ] == .))
  weights[1, ] <- prior$nu / sum(prior$nu)
  
  # constant quantities
  an <- prior$a0 + (n - 1)/2 + K / 2
  
  # sampler
  # pb <- progress_bar$new(total = nsim)
  for (k in 1:nsim) {
    # counts and means per component
    meanj <- map_dbl(1:K, ~ ifelse(nj[.] > 0, mean(y[latent[k, ] == .]), 0))
    # posterior parameters
    kappanj <- nj  + prior$kappa0 
    munj <- (nj * meanj + prior$kappa0 * prior$mu0) / kappanj
    bn <- prior$b0 + 0.5*sum((y - meanj[latent[k, ]])^2) +
      0.5*sum(prior$kappa0 * nj / kappanj * (meanj - prior$mu0)^2)
  
    # sample gamma  
    omega[k + 1] <- rgamma(1, an, bn)
    sigma[k + 1] <-  1 / sqrt(omega[k + 1])
    
    # sample the means
    mu[k + 1, ] <-  map_dbl(1:K, ~
      rnorm(1, munj[.], sigma[k + 1] / sqrt(kappanj[.])))
    
    # allocate again
    latent[k + 1, i] <- latent[k, i] # make a copy
    for (i in 1:n) {
      cnts <- nj + prior$nu - as.integer(latent[k + 1, i] == 1:K)
      wts <- cnts * map_dbl(mu[k + 1, ], ~ dnorm(y[i], ., sigma[k + 1]))
      latent[k+ 1, i] <- sample(K, 1, prob = wts / sum(wts)) # allocate ever individual
    }
    nj <- map_int(1:K, ~ sum(latent[k + 1, ] == .))
    weights[k + 1, ] <- (nj + prior$nu) / sum(nj + prior$nu)
    # pb$tick()
  }
  
  list(mu = mu[-1, ], omega = omega[-1], 
       sigma = sigma[-1], latent = latent[-1, ], weights = weights[-1, ])
}
```


First let's generate data

```{r}
K <- 5
n <- 1000
w_true <- c(0.3, 0.1, 0.2, 0.3, 0.1)
mu_true <- seq(-2.5, 2.5, length.out = 5)
sigma_true <- 0.35
simdata <- map_dbl(1:n, ~{j = sample(K, 1, prob = w_true); rnorm(1, mu_true[j], sigma_true)})
```

And run the sampler
```{r}
nsim <- 1000
set.seed(110104)
prior <- list(mu0 = rnorm(K), a0 = 1, b0 = .01, kappa0 = 5., nu = rep(1, K))
sim7 <- gibbs7(nsim, simdata, prior)
```

We can graph the results

```{r}
{
warmup <- 500
grid_length <- 100
gridy <- seq(-4.5, 4.5, length.out = grid_length)
hist(simdata, breaks = 35, prob = TRUE, xlim = c(-4.5, 4.5),
  col = alpha("blue", 0.15), border = "white", xlab = "simulated data",
  main = "Problem 7: Normal Mixture with K=5 clusters (simulated data)")
mean_dens <- numeric(grid_length)
for (k in (warmup + 1):nsim) {
  dens <- numeric(grid_length)
  for (i in 1:grid_length) {
    dens[i] <- sum(sim7$weights[k, ] * map_dbl(1:K, ~dnorm(gridy[i], sim7$mu[k, .], sim7$sigma[k])))
  }
  mean_dens <- mean_dens + dens / (nsim - warmup)
  lines(gridy, dens, col = alpha("black", 0.005), lw = 3)
}
lines(gridy, mean_dens, col = alpha("black", 0.8), lw = 2)
true_dens <- numeric(grid_length)
for (i in 1:grid_length) {
  true_dens[i] <- sum(w_true * map_dbl(1:K, ~dnorm(gridy[i], mu_true[.], sigma_true)))
}
# lines(density(simdata, bw = sigma_true / 2), col = "magenta")
lines(gridy, true_dens, col = "magenta", lt = 2, lw = 2)
lines(density(simdata, bw = sigma_true / 2), col = "darkgreen", lt = 3, lw = 2)
legend("topright", c("histogram", "posterior mean", "true density", "kde"),
  pch = c(15, NA, NA, NA), lt = c(NA,  1, 2, 3), lw = c(NA, 1, 2, 2),
  col = c(alpha("blue", 0.4), "black", "magenta", "darkgreen"))
}
```

Coincididence table

```{r}
coincidence <- matrix(0, n, n)
for (i in 1:(n - 1)) {
  for (j in (i + 1):n) {
    coincidence[i, j] <- mean(sim7$latent[ ,i] == sim7$latent[ ,j])
  }
}
```

```{r}
image(coincidence, col  = gray((0:32)/32))
```


[Back to Index](#TOC)

----

