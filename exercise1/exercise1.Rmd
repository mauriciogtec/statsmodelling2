---
title: 'Statistical Modelling II'
author: 'Mauricio Garcia Tec'
output: 
  html_notebook: 
    toc: yes
---

$$
\DeclareMathOperator{\N}{N}
\DeclareMathOperator{\Gam}{Gamma}
\DeclareMathOperator{\Wish}{Wishart}
\DeclareMathOperator{\tr}{tr}
$$

```{r setup, include=FALSE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE)
```

# Section one: Preliminaries

## Exchangeability and de Finetti's theorem

A standard situation in statistics is to be presented with a sequence of observations, and use them to make predictions about future observations. In order to do so, we need to make certain assumptions about the nature of the statistical relationships between the sequence of observations.

A common assumption is that our data are **exchangeable**, meaning that their joint probability is invariant under permutations. More concretely, we say a sequence of $N$ observations is finitely exchangeable if 
$$P(X_1\in A_1,X_2\in A_2,\dots, X_N\in A_n) = P(X_{\sigma(1)}\in A_1, X_{\sigma(2)}\in A_2,\dots, X_{\sigma(N)}\in A_n)$$
for any permutation of the integers 1 through $N$, and that an infinite sequence is infinitely exchangeable if this invariance holds for all values of $N$.

### Exercise 1

*Clearly, all iid sequences are exchangeable, but not all exchangeable sequences are iid. Consider an urn, containing $r$ red balls and $b$ blue balls. A sequence of colors is generated by repeatedly sampling a ball from the urn, noting its color, and then returning the ball, plus another ball of the same color, to the urn. Show that the resulting sequence is exchangeable, but not iid.*

*Solution*. They are clearly not iid since the outcome of $X_i$ affects the distribution of $X_{i+1}$. For instance
$$
\begin{aligned}
P(X_2 = r) & = P(X_2 = r \mid X_1 = r) P(X_1 =r) + \\ & \hspace{1cm} P(X_2 = r \mid X_1 = b) P(X_1 =b) \\
    & = \frac{2}{3} \frac{1}{2} + \frac{1}{3} \frac{1}{2} \\ & =  \frac{1}{2}. 
\end{aligned}
$$
Thus
$$
P(X_2 = r) = \frac{1}{2}\neq \frac{2}{3}  = P(X_2 = r \mid X_1 = r).
$$
We now show they're exchangeable. For convenience, denote $R_k$ the number of red and $B_k$ the number of blue outcomes in $x_1, ... x_k$. Observe that by construction
$$
P(X_n = r \mid X_1 = x_1, ..., X_{n_1} = x_{n-1}) = \frac{R_n + 1}{n+1}.
$$
and 
$$
P(X_n = b \mid X_1 = x_1, ..., X_{n_1} = x_{n-1}) = \frac{B_n + 1}{n+1}.
$$
Now, since
$$
p(X_1 , ..., X_n) = p(X_1) p(X_2 \mid X_1) ... p(X_n\mid X_1 , ... X_{n-1}),
$$
we have
$$
\begin{aligned}
P(X_1 = x_1, ..., X_n = x_n) & = \sum_{k=1}^{n} \frac{1}{k+1}\left((R_{k-1} + 1) 1(x_k = r) + (B_{k-1} + 1)1(x_k = b)\right)\\
& = \frac{R_n! B_n!}{(n+1)!}.
\end{aligned}
$$
The latter quantity is invariante under permutations, which shows the exchangeability. 

----------


### De Finneti's theorem


Loosely speaking, de Finetti's Theorem states if a sequence of random variables is infinitely exchangeable, those random variables must be conditionally i.i.d. given some set of parameters. More formally,

```{theorem, name='de Finetti'}
  Let $(X_1,X_2,\dots)$ be an infinite sequence of random variables in some space $\mathcal{X}$. This sequence is infinitely exchangeable if and only if there exists a probability distribution $Q_\theta$, parametrized by some random parameter $\theta\sim \nu$, such that the $X_i$ are conditionally iid given $Q_\theta$ and such that
$$P(X_1\in A_1,X_2\in A_2,\dots) = \int_{\Theta}\prod_{i=1}^\infty Q_\theta(A_i) \nu(d\theta).$$
```

This means we can imagine that any exchangeable sequence has been generated as a sequence of i.i.d.\ random variables with some unknown law. This provides a motivation for Bayesian inference: We have a hierarchical model, where data are generated according to some distribution parametrized by a random (in the Bayesian context -- i.e.\ unknown/uncertain) variable $\theta$, and our uncertainty about $\theta$ is characterized by some distribution $\nu$.

Let's consider the 0/1 form of de Finetti's theorem, for exchangeable sequences of binary variables:


```{theorem, name = 'de Finetti 0/1'}
  An infinite sequence $(X_1,X_2,\dots)$ of binary random variables is exchangeable if and only if its distribution can be written as
  $$\begin{aligned}P(X_1=x_1,X_2=x_2,\dots, X_N=x_N)  =& \int_0^1\prod_{i=1}^N\left\{\theta^{x_i}(1-\theta)^{1-x_i}\right\} d\nu(\theta)\\
    =&\int_0^1 \theta^{k}(1-\theta)^{N-k} d\nu(\theta)\end{aligned}$$
  where $k=\sum_ix_i$.
```

We will now work through (most of) a proof in the next two exercises.

```{exercise}
  We will start off with a finite sequence $(X_1,\dots, X_M)$. For any $N\leq M$, show that
  $$P\left(\sum_{i=1}^N X_i = s\Big|\sum_{i=1}^M X_i = t\right) = \frac{{t\choose s}{M-t\choose N-s}}{{M \choose N}}$$
```

-----

***Solution***. Since the $X_i$ are exchangeable, $P\left(\sum_{i=1}^N X_i = s\Big|\sum_{i=1}^M X_i = t\right)$ boils down to the problem of randomly selecting a subset of size $N$ with exactly $s$ ones from a collection of size $M$ that has $t$ ones and $M-t$ zeros. This is a basic combinatorics problem. There is a total of $\binom{M}{N}$ posible subsets. We can select $s$ ones from the $t$ available ones in a total of $\binom{t}{s}$ ways, and the rest zeros from the available $M-t$, which can be done in $\binom{M-t}{t-s}$ forms. All posibilities have the same probability by exchangeability, so the total number of desired cases divided by the total number of cases is
$$
P\left(\sum_{i=1}^N X_i = s\Big|\sum_{i=1}^M X_i = t\right) = \frac{\binom{t}{s}\binom{M-t}{t-s}}{\binom{M}{N}}.
$$


-----


We can therefore write
\begin{equation}
P\left(\sum_{i=1}^N X_i = s\right) = {N\choose s}\sum_{t=s}^{M-N+s}\frac{(t)_s(M-t)_{n-s}}{(M)_N}P\left(\sum_{i=1}^M X_i = t\right),
\label{eqn:a}
\end{equation}
where $(x)_y = x(x-1)\dots (x-y+1)$.

Let $F_M(\theta)$ be the distribution function of $\frac{1}{M}(X_1, + \dots, + X_M)$  --  i.e.\ a step function between 0 and 1, with steps of size $P(\sum_i X_i= t)$ at $t=0,1,\dots, M$. Then we can rewrite Equation~\ref{eqn:a} as

$$P\left(\sum_{i=1}^N X_i = s\right) = {N\choose s} \int_0^1\frac{(M\theta)_s(M(1-\theta))_{N-s}}{(M)_N}dF_M(\theta)$$
```{exercise}
  Show that, as $M\rightarrow \infty$, we can write
  $$P\left(\sum_{i=1}^N X_i = s\right) \rightarrow {N\choose s} \int_0^1\theta^s(1-\theta)^{N-s}dF_\infty(\theta)$$
```

-------

***Solution***. Observe we can factor out $M$ from every term in the integrand and rewrite it as
$$
\frac{(M\theta)_s(M(1-\theta))_{N-s}}{(M)_N} = 
 \frac{\theta \cdots \left(\theta - \frac{s + 1}{M}\right)(1-\theta)\cdots\left(1-\theta - \frac{N - s + 1}{M}\right)}{\left(1-\frac{1}{M}\right)\cdots\left(1-\frac{N+1}{M}\right)} \xrightarrow{M\to\infty} \theta^s(1-\theta)^{N-s}
$$
We will ignore the formal details of why we can exchange the limit with the integral. We then have
$$
\begin{aligned}
\lim_{M\to\infty} {N\choose s} \int_0^1\frac{(M\theta)_s(M(1-\theta))_{N-s}}{(M)_N}dF_M(\theta)
& =
{N\choose s} \int_0^1 \lim_{M\to\infty}  \frac{(M\theta)_s(M(1-\theta))_{N-s}}{(M)_N} dF_\infty(\theta) \\
& = 
{N\choose s} \int_0^1 \theta^s(1-\theta)^{N-s} dF_\infty(\theta)
\end{aligned}
$$



--------

The proof is completed using a result (the Helly Theorem), that shows that any sequence $\{F_M(\theta); M=1,2,\dot\}$ of probability distributions on [0,1] contains a subsequence that converges to $F(\theta)$.


## Exponential family of distributions


De Finetti's theorem can be seen as a motivation for Bayesian inference. If our data are exchangeable, we know that they are iid according to some unknown probability distribution $F_\theta(X)$, which we can think of as a **likelihood function**, and that they can be represented using an mixture of such iid sequences. As we saw from the 0/1 case, the distribution over probabilities is given by the limit of the empirical distribution function. When not working in this limit, we may choose to model this distribution over the parameters of our likelihood function using a **prior** distribution $\pi(\theta)$ -- ideally one that both assigns probability mass to where we expect the empirical distribution might concentrate, and for which $\int_\Theta F_\theta(X) \pi(d\theta)$ is tractable.

The exponential family of probability distributions is the class of distributions parametrized by $\theta$ whose density can be written as

$$p(x|\theta) = h(x)\exp\{\eta(\theta)^TT(x) - A(\eta(\theta))\}$$
where 

* $\eta(\theta)$ (sometimes just written as $\eta$), is a transformation of $\theta$ that is often referred to as the \textbf{natural or canonical parameter}.
* $T(X)$ is known as a \textbf{sufficient statistic} of $X$. We see that $p(x|\theta)$ depends only on $X$ through $T(X)$, implying that $T(X)$ contains all the relevant information about $X$.
*\item* $A(\eta(\theta)$ (or $A(\eta)$) is known as the \textbf{cumulant function} or the \textbf{log partition function} (remember, a partition function provides a normalizing constant).

```{example, name='The Bernoulli distribution'}
  A Bernoulli random variable $X$ takes the value $X=1$ with probability $\pi$ and $X=0$ with probability $1-\pi$; it's density can be written:

  $$\begin{aligned}
    p(x|\pi) =& \pi^x(1-\pi)^{1-x}\\
    =& \exp\left\{\log\left(\frac{\pi}{1-\pi}\right)x + \log(1-\pi)\right\}\end{aligned}$$
    By rewriting in this exponential family form, we see that
    \begin{itemize}
    \item $\eta = \frac{\pi}{1-\pi}$
    \item $T(x) = x$
    \item $A(\eta) = -\log(1-\pi) = \log(1+e^{\eta})$
    \item $h(x)=1$
    \end{itemize}
```

```{exercise}
  The Poisson random variable has PDF
  $$p(x|\lambda) = \frac{\lambda^xe^{-\lambda}}{x!}$$
  Re-write the density of the Poisson random variable in exponential family form. What are $\eta$, $T(x)$, $A(\eta)$ and $h(x)$? What about if we have $n$ independent samples $x_1,\dots, x_n$?
```

----------

***Solution***. We shall use that $\lambda^x = e^{x\log\lambda}$, then
$$
p(x\mid \lambda) = \frac{1}{x!}e^{x\log\lambda - \lambda}.
$$
We then recognize that $p(x\mid \lambda)$ takes a general exponential form if we set $\theta = \lambda$ and

* $h(x) = 1 / x!$
* $\eta(\lambda) = \log \lambda$
* $T(x) = x$
* $A\colon \eta \to e^\eta$ so that $A(\eta(\lambda)) = \lambda$.

----------

```{exercise}
  The gamma random variable has PDF
  $$p(x|\alpha,\beta) = \frac{\beta^\alpha}{\Gamma(\alpha)}x^{\alpha-1}e^{-\beta x}$$
  What are the natural parameters and sufficient statistics for the gamma distribution, given $n$ observations $x_1,\dots, x_N$?
```

----------

***Solution***. Let us rewrite
$$
p(x\mid \alpha, \beta) =e^{(\alpha - 1)\log x - \beta x + \alpha\log \beta - \log\Gamma(\alpha)},
$$
so we may obtain an exponential form by setting $\theta = (\alpha, \beta)$ and

* $h(x) = 1$
* $\eta(\alpha, \beta) = (\alpha - 1, -\beta)$
* $T(x) = (\log x, x)$
* $A\colon (\eta_1, \eta_2) \to \log\Gamma(\eta_1+1) - (\eta_1 + 1)\log(-\eta_2)$ so that $A(\eta(\alpha, \beta)) = \log\Gamma(\alpha) - \alpha\log\beta$.

----------

### Cumulants and moments of exponential families

We are probably most familiar with using the PDF or the CDF of a random variable to describe its distribution, but there are other representations that can be useful. The \textbf{moment generating function} $M_X(s) = E[\exp(s^Tx)] = \int_\mathcal{X} e^{s^Tx}p_X(x) dx$ is the Laplace transform of the PDF $p_X(x)$. As the name suggests, we can use the moment-generating function to generate the (uncentered) moments of a random variable; the $n$th moment is given by

$$m_n = \frac{d^nM_X}{ds^n}\Bigr|_{s=0}$$

```{exercise}
  For exponential family random variables, we know that the sufficient statistic $T(X)$ contains all the information about $X$, so (for univariate $X$) we can write the moment generating function of the sufficient statistic as $E[e^{sT(X)}|\eta]$. Show that the moment generating function for the sufficient statistic of  an arbitrary exponential family random variable with natural parameter $\eta$ can be written as
    $$M_{T(X)}(s) = \exp\{A(\eta+s) - A(\eta)\}$$
```

------------

***Solution***.  Essentially, what is happening is that the argument $s$ of the MGF becomes a shift on the Laplace transform of the sufficient statistic. We compute
$$
\begin{aligned}
E\left[e^{sT(X)}\right] & = \int_{-\infty}^\infty e^{sT(x)}e^{\eta T(x) - A(\eta)} \; dx \\
& = e^{- A(\eta)} \int_{-\infty}^\infty e^{(s + \eta)T(x)} \; dx \\
& = e^{- A(\eta) + A(s + \eta)}   \int_{-\infty}^\infty p_X(x) \; dx
\\
& = e^{- A(\eta) + A(s + \eta)}.
\end{aligned}
$$

-----

A related representation is the \textbf{cumulant generating function} $C_X(s) = \log E[e^{s^Tx}] = \log(M_X(s))$. Clearly, for exponential families this takes the form $C_{T(X)}(s) = A(\eta+s)-A(\eta)$. This explains why $A(\eta)$ is sometimes called the cumulant function! The cumulant function can be used to generate the cumulants of a distribution as

$$\kappa_n = \frac{d^nC_X}{ds^n}\Bigr|_{s=0}$$

The first three cumulants are the same as the first three central moments of the distribution -- meaning, the cumulant generative function is a useful tool for calculating mean, variance and the third central moment.

```{exercise}
  It is usually easier to calculate mean and variance using the cumulant generating function rather than the moment generating function. Starting from the exponential family representation of the Poisson distribution from Exercise 1.4, calculate the mean and variance of the Poisson using a) the moment generating function, and b) the cumulant generating function.
```

----

***Solution***. For the poisson distribution, $T(x) = x$, so the MGF of the sufficient statistics is just the MGF of $X$. We saw earlier that for the Poisson PDF we have $\eta = 
\log \lambda$ and $A(\eta) = e^\eta$. Thus
$$
M_X(s) = e^{A(\log\lambda + s) - A(\log\lambda)} =  e^{\lambda e^s - \lambda}.
$$
The first and second moments are
$$
E[X] = M_X'(0) = \lambda e^s e^{\lambda e^s - \lambda}\big\rvert_{s = 0} = \lambda
$$
$$
E[X^2] = M_X''(0) = \lambda (1 + \lambda e^s) e^{s + \lambda e^s - \lambda}\big\rvert_{s = 0} = \lambda(1 + \lambda) = \lambda + \lambda^2,
$$
Thus, the variance is 
$$
\mathrm{Var}(X) = E[X^2] - (E[X])^2 = \lambda.
$$
Now the easier CGF version.
$$
E[X] = C'_X(0) = \lambda e^s \big\rvert_{s = 0} = \lambda
$$
and
$$
\mathrm{Var}(X)  = C''_X(0) =  \lambda e^s \big\rvert_{s = 0} = \lambda.
$$

----



### Conjugate priors

Exponential families are very important in Bayesian statistics because, for any exponential family likelihood, we can find an conjugate exponential family prior. If our likelihood takes the form

$$f(x|\eta) = h(x)\exp\left\{\eta^TT(x) - A(\eta)\right\}$$

then a conjugate prior is given by

$$p(\eta|\xi,\nu) = g(\xi,\nu)\exp\left\{\eta^T\xi - \nu A(\eta)\right\}$$

Below are some exercises based on common conjugate priors.

```{exercise}
  Suppose we have $N$ independent observations $x_1,\dots, x_N \stackrel{\small{iid}}{\sim}\mbox{Normal}(\mu, \sigma^2)$. If $\sigma^2$ is known and $\mu\sim \mathrm{Normal}(\mu_0,\sigma_0^2)$, derive the posterior for $\mu|x_1,\dots, x_N$
```

----

***Solution***. For convenience, we'll work with the notation (of the precision) $\tau = 1/\sigma^2$ and $\tau_0 = \sigma_0^2$. Then using Bayes' rule
$$
\begin{aligned}
p(\mu \mid x) & \propto p(x \mid \mu) p(\mu) \\
& \propto \exp\left\{-\frac{\tau}{2}\sum_{i=1}^N(x_i - \mu)^2 \right\}  \exp\left\{ - \frac{\tau_0}{2}(\mu - \mu_0)^2 \right\} \\
& \propto \exp\left\{-\frac{1}{2}\left((N\tau + \tau_0)\mu^2 - 2(N\tau \bar{x} + \tau_0 \mu_0) \mu \right)\right\} \\
& \propto \exp\left\{ -\frac{(N\tau + \tau_0)}{2}\left(\mu - \frac{N\tau\bar{x} + \tau_0\mu_0}{N\tau + \tau_0} \right)^2 \right\}.
\end{aligned}
$$
Thus
$$
\mu \mid x \sim \mathrm{Normal}\left(\frac{N\tau}{N\tau + \tau_0}\bar{x} + \frac{\tau_0}{N\tau + \tau_0}\mu_0, N\tau + \tau_ 0\right).
$$
The last expression is beautiful because we see that the precision increases linearnly with the number of data points, and that the new expected mean is a weighted average of the prior and the empirical mean using the precision as weights. 

----

```{exercise}
  Now, let's assume  $x_1,\dots, x_N \stackrel{\small{iid}}{\sim}\mbox{Normal}(\mu, \sigma^2)$ with known mean $\mu$ but unknown variance $\sigma^2$. Let's express the likelihood in terms of the precision, $\omega=1 / \sigma^2$:
  $$f(x_i|\mu, \omega) = \sqrt{\frac{\omega}{2\pi}} \exp\left\{-\frac{\omega}{2}(x_i-\mu)^2\right\}$$
  Let $\omega$ have a gamma prior (this is also known as putting an inverse-gamma prior on $\sigma^2$):
  $$p(\omega) = \frac{\beta^\alpha}{\Gamma(\alpha)}\omega^{\alpha-1}e^{-\beta \omega}$$
  Derive the posterior distribution for $\omega$
```

----

***Solution***. We procede similarly as above. 
$$
\begin{aligned}
p(\tau \mid x) & \propto  p(\tau \mid x) p(\tau)  \\
& \propto \tau^{N/2}\exp\left\{-\frac{\tau}{2}\sum_{i=1}^N(x_i - \mu)^2 \right\} \tau^{\alpha - 1}\exp\{-\beta\tau\} \\
& = \tau^{\alpha + N/2 - 1} \exp\left\{-\left(\frac{1}{2}\sum_{i=1}^N(x_i - \mu)^2  + \beta \right)\tau \right\}.
\end{aligned}
$$
Hence,
$$
\sigma^2 \mid x \sim \mathrm{InvGamma}\left(\alpha + \frac{N}{2}, \frac{1}{2}\sum_{i=1}^N(x_i - \mu)^2  + \beta \right).
$$

----


```{exercise}
  Let's assume $x \sim \mbox{Normal}(0, \sigma^2)$ and that $\sigma^2\sim\mbox{InvGamma}(\alpha,\beta)$ (i.e.\ $1/\sigma^2 \sim \mbox{Gamma}(\alpha,\beta)$). Show that the marginal distribution of $x$ is given by a Student's $t$ distribution.
```

---

***Solution***. First we define the general form a univariate general t-distribution of a random variable $x$, depending on location and precision matrix parameters $\mu$ and $\omega$, and $\nu$ degrees of freedom. We emphasize that $1 / \omega$ will not the exact variance as in the Normal case (but related):

$$
\mathrm{tStudent(x; \mu, \omega, \nu)} = \frac{\Gamma\left(\frac{\nu + 1}{2}\right)}{\sqrt{\nu\pi}\Gamma\left(\frac{\nu}{2}\right)} \left(1 + \omega\frac{x^2}{\nu}\right)^{-\frac{\nu + 1}{2}}.
$$
One can verify that the distribution converges to a $\mathrm{Normal}(\mu, \omega)$ as $\nu \to \infty$.

We now show the marginal of $x$ is a generalised t-distribution. We compute
$$
\begin{aligned}
p(x) &= \int p(x \mid \omega)p(\omega) \; d\omega \\
     & \propto \int \exp\left\{ -\left(\frac{x^2}{2} + \beta\right)\omega\right\}\omega^{\alpha + 1/2 - 1} \; d\omega \\
      & \propto \left(\frac{x^2}{2} + \beta\right)^{-\alpha - 1/2} \\
      & \propto \left(1 + \frac{\alpha}{\beta}\cdot\frac{x^2}{2\alpha}  \right)^{-\frac{2\alpha + 1}{2}} \\
      & \propto \mathrm{tStudent}\left(x; 0, \frac{\alpha}{\beta}, 2\alpha\right).
\end{aligned}
$$
So $x$ is a generalised $t$ with mean zero, precision $\alpha/\beta$, and $2\alpha$ degrees of freedom.

---


## Multivariate Normal Distribution

So far, we have looked at univariate random variables - particularly, the univariate normal random variable, which is characterized by its mean and variance. We will often work with the multivariate normal distribution, a natural generalization characterized by a mean vector and a covariance matrix.

```{exercise, name = 'covariance matrix'}
  The covariance matrix $\Sigma$ of a vector-valued random variable $x$ is the matrix whose entries $\Sigma(i,j) = cov(x_i,x_j)$ are given by the covariance between the $i$th and $j$th elements of $x$, giving

  $$\Sigma = E\left[(x-\mu)(x-\mu)^T\right]$$

  Show that a) $\Sigma = E[xx^T] - \mu\mu^T$; b) if the covariance of $x$ is $\sigma$, then the covariance of $Ax+b$ is $A\Sigma A^T$
```

----

***Solution***. By the definition of entrywise expectation and matrix operations we have
$$
\begin{aligned}
(E[xx'] - \mu\mu')_{ij} & = E[(xx')_{ij}] -  (\mu\mu')_{ij} \\
& = E[x_ix_j] - \mu_i\mu_j \\
& = \Sigma_{ij}.
\end{aligned}
$$
So $E[xx'] - \mu\mu' = \Sigma$, this proves (a). For (b), define $y=Ax + b$, observe first that by the linearity and entrywise definition of the expectation operator we have
$$
E[y] = E[Ax + b] = A\mu + b.
$$
Hence,
$$
y - E[y] = (Ax + b) - (A\mu + b) = A(x-\mu).
$$
Therefore, again using the linearity of the expectation, we conclude
$$
\begin{aligned}
E[(y - E[y])(y - E[y])'] & = E[A(x-\mu)(A(x-\mu))'] \\
  & = AE[(x - \mu)(x-\mu)']A' \\
  & = A\Sigma A'
\end{aligned}
$$

----

```{exercise, name = 'Standard multivariate normal'}
  The simplest multivariate normal, known as the standard multivariate normal, occurs where the entries of $x$ are independent and have mean 0 and variance 1. a) What is the moment generating function of a univariate normal, with mean $m$ and variance $v^2$? b) Express the PDF and moment generating function of the standard multivariate normal, in vector notation.
```

----

***Solution***. (a) We can directly compute the Laplace transform
$$
\begin{aligned}
M_X(s) = E[e^{s X}] & = \frac{1}{\sqrt{2\pi}}\int_{-\infty}^\infty e^{sx}e^{\frac{1}{2}x^2}\;dx \\ 
& = e^{\frac{1}{2}s^2}\frac{1}{\sqrt{2\pi}}\int_{-\infty}^\infty e^{\frac{1}{2}(x-s)^2}\;dx \\
& = e^{\frac{1}{2}s^2}.
\end{aligned}
$$
For (b), let $X\sim \mathrm{Normal}(0, I_n)$ be an n-dimensional standard Normal. For any vector $s\in\mathbb{R}^n_{\geq 0}$, the MGF is
$$
\begin{aligned}
M_X(s) & = E[e^{s'X}] \\ 
  & = \prod_i E[e^{s_iX_i}] \\
  & = \prod_i e^{\frac{1}{2}s_i^2} \\
  & = e^{\frac{1}{2}s's}.
\end{aligned}
$$
Now, for the PDF we use the independence assumption to write the joing as product of the marginals 
$$
\begin{aligned}
p_X(x)  & = \prod_i p_{X_i}(x_i) \\
  & = \prod_i \frac{1}{\sqrt{2\pi}} e^{\frac{1}{2}x_i^2} \\ 
  & = \frac{1}{(2\pi)^{n/2}} e^{\frac{1}{2}\sum_i x_i^2} \\ 
  & =  \frac{1}{(2\pi)^{n/2}} e^{\frac{1}{2}x'x}.
\end{aligned}
$$

----

```{exercise, name = 'Multivariate normal'}
  A random vector $x$ has multivariate normal distribution if and only if every linear combination of its elements is univariate normal, i.e.\ if the scalar value $z = a^Tx$ is normally distributed for all possible $x$. Prove that this implies that $x$ is multivariate normal if and only if its moment generating function takes the form $M_X(s) = \exp\{s^T\mu + s^T\Sigma s / 2\}$, where $\mu$ and $\Sigma$ are the mean and covariance of $x$. \textit{Hint: We know the moment generating function of $z$ in terms of the mean and variance of $z$, from the previous question...}
```

----

***Solution***. We first show that the definition implies the required form of the MGF. Let $\mu = E[X]$ and $\Sigma = \mathrm{Cov}(X)$. Independently of the distribution of $X$, the properties of expectation imply that $E[a'X] = a'\mu$ and $\mathrm{Var}(a'X) = a'\Sigma a$. 

I will also show that for any $Y \sim N(\mu, \sigma^2)$, the MGF is
$$
M_Y(s) = e^{\mu s+ \frac{s^2\sigma^2}{2}}.
$$
To prove the above, we use the previous exercise for the standard normal and rewrite
$$
M_Y(s)= E[e^{sY}] = e^{s\mu}E[e^{(s\sigma)\frac{Y - \mu}{\sigma}}] =  e^{s\mu} e^{\frac{s^2\sigma^2}{2}}.
$$
Now we go back to the proof. Assuming $s'X$ is Normal for any $s\in\mathbb{R}^n$, then $s'X \sim \mathrm{Normal}(s'\mu, s'\Sigma s)$, so
$$
M_{X}(s) := E[e^{s'X}] = M_{s'X}(1) = e^{s'\mu + \frac{1}{2}s'\Sigma s}. 
$$
We now need to prove the converse. Assume the MGF takes the specified form and let $a\in\mathbb{R}^n$ be any vector. Then
$$
M_{a'X}(s) = M_X(sa) = e^{(sa)'\mu + \frac{1}{2}(as)'\Sigma(as)} = e^{s (a'\mu) + \frac{1}{2} s^2 (a'\Sigma a)}. 
$$
From the uniqueness of the MGFs, it follows that $a'X \sim \mathrm{Normal}(a'\mu, a'\Sigma a)$.

----


```{exercise, name = 'Relationship to standard multivariate normal'}
  An equivalent statement is that a random vector $x$ has multivariate normal distribution if and only if it can be written in the form
  $$x = Dz + \mu$$
  for some matrix $D$, real-valued vector $\mu$, and vector $z$ distributed according to a standard multivariate normal. Express the moment generating function of $x$ in terms of $D$, and uncover the relationship between $D$ and $\Sigma$. Use this result to suggest a method for generating multivariate normal random variables, if you have a method for generating Normal(0,1) univariate random variables.
```

----

***Solution***. We use that 
$$
s'X = s'(DZ + \mu)= t'Z + s'\mu \quad \text{ with } t = D's.
$$
Hence
$$
M_X(s) = e^{s'\mu}M_Z(t) = e^{s'\mu + \frac{1}{2}t't} = e^{s'\mu + \frac{1}{2}s'(DD')s}.
$$
From here, we see that $DD'$ must satisfy $DD' = \Sigma$.

This suggests the following approach for generating multivariate normal variables $X\sim \mathrm{Normal}(\mu, \Sigma)$.

1. Find a factorization (e.g., using Cholesky) $\Sigma = DD'$.
2. Generate independent standard normal $Z$
3. Return $X = DZ + \mu$.

----

```{exercise}
  Use the result from the previous question to show that the PDF of a multivarite normal random vector $x\sim\mbox{Normal}(\mu, \Sigma)$ takes the form

  $$p(x) = \frac{1}{(2\pi)^{n/2}}|\Sigma|^{-1/2}\exp\left\{-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)\right\},$$
by using a change-of-variables from the standard multivariate normal distribution.
```

----

***Solution***. For this we'll need $D$ to be invertible, which is always possible as long as $\Sigma$ is invertible.

The transformation $g\colon: Z\mapsto DZ + \mu$ has Jacobian Matrix $D$, so the change of volume factor is 
$$
\left\lvert \frac{d}{dx}g^{-1}(x)\right\rvert = \lvert D \rvert ^{-1} = (\lvert D \rvert^2) ^{-1/2} = \lvert\Sigma\rvert ^{-1/2},
$$
where we used standard properties of the determinant (determimnant of the transpose is the smame, determinant of product of invertible matrices is the product of determinants, etc.). Now, using the change of variable formula we get
$$
\begin{aligned}
f_X(x) & = \lvert \Sigma \rvert  ^{-1/2}  f_Z(g^{-1}(x))  \\ 
& = (2\pi)^{-n/2} \lvert \Sigma \rvert  ^{-1/2} e^{-\frac{1}{2}(D^{-1}(x - \mu))'(D^{-1}(x - \mu))} \\
& = (2\pi)^{-n/2} \lvert \Sigma \rvert  ^{-1/2} e^{-\frac{1}{2}(x - \mu)'(DD')^{-1}(x - \mu)} \\
& = (2\pi)^{-n/2} \lvert \Sigma \rvert  ^{-1/2} e^{-\frac{1}{2}(x - \mu)'\Sigma^{-1}(x - \mu)}.
\end{aligned}
$$

----


### Manipulation of multivariate normals

Like its univariate counterpart, the multivariate normal distribution is closed under a number of operations, which we will explore here.

```{exercise, name = 'marginal distribution'}
  Let us assume that $x\sim \mbox{Normal}(\mu, \Sigma)$, and let us partition $x$ into 2 components $x_1$ and $x_2$. Let us similarly partition $\mu$ and $\sigma$ so that

  $$\mu  = (\mu_1, \mu_2)^T \qquad \qquad \Sigma = \begin{pmatrix}\Sigma_{11} & \Sigma_{12} \\ \Sigma_{21} & \Sigma_{22}\end{pmatrix} = \begin{pmatrix}\Sigma_{11} & \Sigma_{12} \\ \Sigma_{12}^T & \Sigma_{22}\end{pmatrix}$$

  Derive the marginal distribution of $x_1$.
```


----

***Solution***. We choose $a = (1,...,1,0,...0)$ where there is a one for each entry of block noe and a zero for each entry of block two. Then $a'x = x_1$. We know $a'x = x_1$ has to be Normal with mean $a'\mu = \mu_1$ and variance $a'\Sigma a = \Sigma_{11}$. 

----


```{exercise, name = 'Precision matrix'}
  Earlier, we chose to express a univariate normal random variable in terms of its precision, to make math easier. We can also express a multivariate normal in terms of a precision matrix $\Omega = \Sigma^{-1}$. Partition $\Omega$ as
  $$\Omega = \begin{pmatrix}\Omega_{11} & \Omega_{12} \\ \Omega_{12}^T & \Omega_{22}\end{pmatrix}$$
  and express $\Omega_{11}$, $\Omega_{12}$ and $\Omega_{22}$ in terms of $\Sigma_{11}$, $\Sigma_{12}$ and $\Sigma_{22}$. \textit{Hint: You'll need the matrix inversion lemma}
```

----

***Solution***. We require
$$
\Omega \Sigma = \Sigma \Omega = I.
$$
which by block multiplication gives
$$
\Omega_{11} \Sigma_{11} + \Omega_{12} \Sigma_{21} = \Omega_{12} \Sigma_{21} + \Omega_{22} \Sigma_{22} = I \\
\Omega_{21} \Sigma_{11} + \Omega_{22} \Sigma_{21} = \Omega_{11} \Sigma_{12} + \Omega_{12} \Sigma_{22} = 0 
$$
Also we have a symmetry condition
$$
\Omega_{12} = \Omega_{21}'.
$$
After some standard manipulations, we solve the linear system and obtain
$$
\Omega_{11} = \Sigma_{11} - \Sigma_{12}\Sigma^{-1}_{22}\Sigma_{12}' \\
\Omega_{22} = \Sigma_{22} - \Sigma_{21}\Sigma^{-1}_{11}\Sigma_{21}'
$$
and
$$
\Omega_{21} = -\Sigma_{22}^{-1}\Sigma_{21}\Omega_{11} \\
\Omega_{12} = \Omega_{21}' = - \Omega_{11}\Sigma_{12}\Sigma_{11}^{-1}.
$$

----

```{exercise, name = 'Conditional distribution'}
The conditional distribution of $x_1|x_2$ is also normal, with mean $\mu_1+\Sigma_{12}\Sigma_{22}^{-1}(x_2-\mu_2)$ and covariance $\Sigma_{11} - \Sigma_{12}\Sigma_{22}^{-1}\Sigma_{12}^T$. Prove this for the case where $\mu$ is zero (the general case isn't really harder, just more tedious). \textit{Hint: ignore any constants that don't involve $x_1$. You might want to work with the log conditional density.}
```

----

***Solution***. We leverage the precision matrix notation and the results from the previous exercises, and the completing the square formula for the matrix case.
$$
\begin{aligned}
p(x_1 \mid x_2) & \propto p(x_1, x_2) \\
& \propto \exp\left\{ - \frac{1}{2} (x_1,x_2)'\Omega (x_1, x_2) \right\} \\
& = \exp\left\{- \frac{1}{2} (x_1,x_2)'\begin{pmatrix} \Omega_{11} & \Omega_{12} \\ \Omega_{21} & \Omega_{22} \end{pmatrix} (x_1, x_2) \right\} \\
& \propto \exp\left\{ - \frac{1}{2} \left(x_1'\Omega_{11}x_1 + 2 x_1'\Omega_{12}x_2 \right) \right\} \\
 & \propto \exp\left\{- \frac{1}{2}(x_1 + \Omega_{11}^{-1}\Omega_{12}x_2)'\Omega_{11}(x_1 + \Omega_{11}^{-1}\Omega_{12}x_2) \right\} \\
 & \propto \exp\left\{-  \frac{1}{2} (x_1 - \Sigma_{12}\Sigma_{11}^{-1}x_2)'\Omega_{11}(x_1 - \Sigma_{12}\Sigma_{11}^{-1}x_2) \right\}.
\end{aligned}
$$
Therefore
$$
x_1 \mid x_2 \sim \mathrm{Normal}\left(\Sigma_{12}\Sigma_{11}^{-1}x_2, \Omega_{11} \right).
$$
where $\Omega_{11}$ is the precision we computed earlier $\Omega_{11} = \Sigma_{11} - \Sigma_{12}\Sigma^{-1}_{22}\Sigma_{12}'$. 

----


```{exercise, name = 'Conjugacy'}
If $x\sim \mbox{Normal}(\mu, \Sigma)$ and $\mu \sim \mbox{Normal}(\mu_0,\Sigma_0)$, derive the posterior of $\mu|x$ \textit{again, ignoring normalizing constancts will make this easier}.
```
  
----

**Solution***. Direct computation with Bayes' rule
$$
\begin{aligned}
p(\mu \mid x) & \propto p(x \mid \mu)p(\mu) \\
  & \propto \exp\left\{- \frac{1}{2}(x-\mu)'\Omega (x -\mu) - \frac{1}{2}(\mu-\mu_0)'\Omega_0(\mu - \mu_0)\right\} \\
    & \propto \exp\left\{- \frac{1}{2}\left( \mu'(\Omega + \Omega_0)\mu - 2(\Omega x + \Omega_0\mu_0)'\mu  \right)\right\} \\
        & \propto \exp\left\{- \frac{1}{2}\left(\left(\mu - (\Omega + \Omega_0)^{-1}(\Omega x + \Omega_0 \mu_0)\right)' (\Omega + \Omega_0) \left(\mu - (\Omega + \Omega_0)^{-1}(\Omega x + \Omega_0 \mu_0)\right)\right)\right\}.
\end{aligned}
$$
Hence
$$
\mu \mid x \sim \mathrm{Normal}\left((\Omega + \Omega_0)^{-1}(\Omega x + \Omega_0 \mu_0), \Omega + \Omega_0 \right)
$$
Again, this a nice expression when interpreted as weighed means.

----


## Frequentist estimation and uncertainty quantification

In this section, we're going to go over basic frequentist approaches to inference, with a focus on multiple linear regression (since we're next going to look at Bayesian regression). Some of this should be familiar to you, although we will go into quite some depth. Throughout the remainder of this section, we are going to assume our data follow a linear model, of the form
$$y_i = x_i^T\beta + \epsilon_i,\qquad i=1,\dots,N$$

There are a number of options for estimating $\beta$. Three commonly used techniques are:

1. **Method of Moments**: Select $\hat{\beta}$ so that the empirical moments of the observations match the theoretical moments.
2. **Maximum likelihood**: Assume a model for generating the $\epsilon_i$, and find the value of $\hat{\beta}$ that maximizes the likelihood.
3. **Loss function**: Construct a loss function between the $y_i$ and $x_i^T\hat{\beta}$, and minimize that loss function.


```{exercise, name = 'method of moments'}
  To obtain the theoretical moments, we can assume that $E[y_i|x_i] = x_i^T\beta$, implying that the covariance between the predictors $x_i$ and the residuals is zero. By setting the sample covariance between the $x_i$ and the $\epsilon_i$ to zero, derive a method of moments estimator $\hat{\beta}_{MM}$
```

----

***Solution***. For this problem we assume $x_i$ is also random. The assumption $E[y_i \mid x_i] = x_i'\beta$ is equivalent to $E[\epsilon_i x_i] = \mathrm{Cov}(x_i, \epsilon_i) = 0$ (here $x_i$ is a vector and $\epsilon_i$ a scalar, so it's a lazy notation for saying it holds for every entry of $x_i$). To show the $E[y_i \mid x_i] = x_i'\beta$ imples $E[\epsilon_i x_i] = 0$, we verify that
$$
\begin{aligned}
E [x_i \epsilon_i] & = E[x_i (y_i - x_i'\beta)] \\
& = E[x_i (y_i - E[y_i\mid x_i])] \\ & = 
 E [ E [x_i (y_i - E[y_i\mid x_i])\mid x_i] ] \\
 & = E[x_i(E[y_i \mid x_i] - E[y_i\mid x_i])] \\
 & = 0.
\end{aligned}
$$
We can now use the method of moments estimator
$$
0 = \frac{1}{n}\sum_{i = 1}^n x_i(y_i - x_i'\beta)
$$
Which is equivalent to
$$
X'Y = \sum_i y_i x_i = \sum_i x_i x_i' \beta = X'X\beta,
$$
where $X$ is the matrix with $x_i'$ in its $i$-th row and $Y$ the vector with $y_i$ in its $i$-th entry. Finally, assuming $X'X$ is full rank, we obtain $\hat{\beta}_{MM}$ (which is the same as the OLS estimator)
$$
\hat{\beta}_{MM} = (X'X)^{-1}X'Y.
$$

----


```{exercise, name = 'maximum likelihood'}
  Show that, if we assume $\epsilon_i\sim \mbox{Normal}(0,\sigma^2)$, then the ML estimator $\hat{\beta}_{ML}$ is equivalent to the method of moments estimator.
```

---

***Solution***. To maximise the likelihood, we can find the argument that maximises the loglikelihood, which for the normal PDF is
$$
\log p(y ; \beta, \sigma^y) = -\frac{1}{2\sigma^2}\vert Y - X\beta \rVert^2 - \frac{n}{2}\log(2\pi\sigma^2).
$$
We can see beforehand that the minimiser on $\beta$ will not depend on $\sigma^2$, since it dependens only on the quadratic term $\lVert y - X\beta \rVert^2$.  We can use calculus (in matrix form) to solve
$$
0 = \nabla_\beta \log p(y; \beta, \sigma^2) = -\frac{1}{2\sigma^2}(2X'X\beta - 2Y'X),
$$
obtaining
$$
\hat{\beta}_{ML} = (X'X)^{-1}X'Y.
$$
As predicted, the ML estimator of $\beta$ does not depend on $\sigma^2$ (different variance assumptions would lead to the same estimator of $\beta$).

---


```{exercise, name = 'Least squares loss function'}
  Show that if we assume a quadratic loss function, i.e.\ $\hat{\beta}_{LS} = \arg\min_{\beta\in \mathbb{R}^p}\sum_{i=1}^N(y_i - x_i^T\beta)^2$, we recover the same estimator again.
```


---

***Solution***. Here
$$
\sum_{i=1}^N(y_i - x_i^T\beta)^2 = \Vert y - X\beta \rVert^2 = -2\sigma^2 \log p (y; \beta, \sigma^2),
$$
so minimising the quadratic loss is equivalent to minimising the loglikelihood we we showed in the previous exercise, leading to the same solution.

---

```{exercise, name = 'Ridge regression'}
  We may wish to add a regularization term to our loss term. For example, ridge regression involves adding an L2 penalty term, so that
  $$\hat{\beta}_{\small{ridge}} = \arg\min_{\beta\in \mathbb{R}^p}\sum_{i=1}^N(y_i - x_i^T\beta)^2 \; s.t. \; \sum_{j=1}^p \beta_j^2 \leq t$$
  for some $t\geq 1$.

  Reformulate this constrained optimization using a Lagrange multiplier, and solve to give an expression for $\hat{\beta}_{\small{ridge}}$. Comparing this with the least squares estimator, comment on why this estimator might be prefered in practice.
```


----

***Solution***. The Lagrange function associated with this optimization problem is
$$
L(\beta, \lambda) = \lVert Y - X\beta\rvert^2 + \lambda(\lVert \beta \rVert^2 - t).
$$
We see that
$$
\arg\min_\beta L(\beta, \lambda) = \arg\min_\beta \lVert Y - X\beta\rvert^2 + \lambda \lVert \beta \rVert^2.
$$
This is true since $t$ is constant and the term $-\lambda t$ dissappears when minimising with respect to $\beta$ only. The last expression is the usual form of the loss function for Ridge regression. We see that if we solve for the minimum
$$
0 = \nabla_\beta (\lVert Y - X\beta\rvert^2 + \lambda \lVert \beta \rVert^2) = 2X'X\beta - 2Y'X\beta + 2\lambda\beta,
$$
we obtain
$$
\hat{\beta}_\text{Ridge} = (X'X + \lambda I)^{-1}X'Y.
$$
The added term $\lambda I$ acts as a contraction, the larger it is, the more $\hat{\beta}_\text{Ridge}$ gets pulled towards zero, as compared to the least squares estimator.


----
