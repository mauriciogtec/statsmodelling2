---
title: 'Statistical Modelling II'
author: 'Mauricio Garcia Tec'
output: 
  html_notebook: 
    toc: yes
---

$$
\DeclareMathOperator{\N}{N}
\DeclareMathOperator{\Gam}{Gamma}
\DeclareMathOperator{\Wish}{Wishart}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\Po}{Poisson}
\DeclareMathOperator{\Prob}{P}
\DeclareMathOperator{\cov}{Cov}
\newcommand{\D}[1]{\mathrm{#1}}
$$

[Return to main Index](https://github.com/mauriciogtec/statsmodelling2)

```{r setup, include=FALSE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE)
```


# Section 4: Gaussian Processes

## Non-linear functions

### Regression view

So far, we've assumed our latent function is a linear function of our data -- which is obviously limiting. One way of circumventing this is to project our inputs into some high-dimensional space using a set of basis functions $\phi:\mathbb{R}^d\rightarrow \mathbb{R}^N$, and then performing linear regression in that space, so that

$$y_i = \phi(x)^T\beta + \epsilon_i$$

For example, we could project $x$ into the space of powers of $x$, i.e. $\phi(x) = (1,x,x^2,x^3\dots)$ to obtain polynomial regression.

### Exercise 1

*Let $\mathbf{y}$ and $\mathbf{X}$ be set of observations and corresponding covariates, and $y_*$ be the unknown value we wish to predict at covariate $\mathbf{x}_*$.  Assume that
  $$\begin{aligned}
    \beta \sim& \mbox{N}\left(0,\Sigma\right)\\
    \begin{bmatrix}f_* \\ \mathbf{f}\end{bmatrix} =& \begin{bmatrix}\boldsymbol{\phi}_*^T\\ \boldsymbol{\Phi}\end{bmatrix}\beta\\
      \begin{bmatrix}y_* \\ \mathbf{y}\end{bmatrix} \sim& \mbox{N}\left(\begin{bmatrix}f_* \\ \mathbf{f}\end{bmatrix},\sigma^2\mathbf{I}\right)
      \end{aligned}$$
  where $\boldsymbol{\phi}:=\phi(\mathbf{x})$ and $\boldsymbol{\Phi}:=\phi(\mathbf{X})$.
  What is the predictive distribution $p(f_*|\mathbf{y},\mathbf{x}_*,\mathbf{X})$? Note: this is very similar to questions we did in Section 1.*
 
*Solution*. Out model matrix is $\Phi$ and the new point is $\phi_*$, we can ignore the $X$ notation. The posterior predictive distribution from a linear regression is
$$
p(\beta \mid y, \Phi)  = N\left(\beta \;\bigg\vert\; (\Phi'\Phi + \sigma^2\Sigma^{-1})^{-1}\Phi'y, \frac{1}{\sigma^2}(\Phi'\Phi + \Sigma^{-1})^{-1}\right)
$$
Note that $f_* = \phi_* \beta$. Hence $f_*$ is also Gaussian and

$$
E(f_* \mid y, \Phi, \phi_*) = \phi_*'(\Phi'\Phi + \sigma^2\Sigma^{-1})^{-1}\Phi'y
$$
and
$$
\mathrm{Var}(f_* \mid y, \Phi, \phi_*) = \sigma^2\phi_*'(\Phi'\Phi + \sigma^2\Sigma^{-1})^{-1}\phi_*
$$

[Back to Index](#TOC)

Note that, in the solution to Exercise 1, we only ever see $\phi$ or $\Phi$ in a form such as $\Phi^T\Sigma\Phi$. We will define $k(\mathbf{x},\mathbf{x}') = \phi(\mathbf{x})^T\Sigma\phi(\mathbf{x}')$. Since $\Sigma$ is positive definite, we can write:

$$k(\mathbf{x},\mathbf{x}') = \psi(\mathbf{x})^T\psi(\mathbf{x})$$
where $\psi(\mathbf{x}) = \phi(\mathbf{x})\Sigma^{1/2}$

If (as here) we only ever access $\psi$ via this inner product, we can choose to work instead with $k(\cdot,\cdot)$. This may be very convenient if the dimensionality of $\psi(x)$ is very high (or even infinite... see later). $k(\cdot,\cdot)$ is often refered to as the kernel, and this replacement is referred to as the kernel trick.

### Exercise 2

Let's look at a concrete example, using the old faithful dataset on $R$

* `data("faithful",package="datasets")` in R
* or available as `faithful.csv` on github if you're not using R.
\end{itemize}

Let $\phi(x) = (1,x,x^2,x^3)$. Using appropriate priors on $\beta$ and $\sigma^2$, obtain a posterior distribution over $f:=\phi(x)^T\beta$. Plot the function (with a 95% credible interval) by evaluating this on a grid of values.
  
```{r}
library(rstan)
library(tidyverse)
```
  
```{r}
data("faithful",package="datasets")

```

```{r}
X <- faithful$waiting
y <- faithful$eruptions
Phi <- faithful %>% 
  model.matrix(~ waiting + I(waiting^2 )+ I(waiting^3), data = .)
```

Here's the code for a standard regression

```{r}
stan2_code <- '
data {
  int<lower=0> N;
  int<lower=0> D;
  matrix[N, D] X;
  vector[N] y;
  real<lower=0> kappa;
  real<lower=0> a;
  real<lower=0> b;
  int<lower=0> N_grid;
  matrix[N_grid, D] X_grid;
}
parameters {
  vector[D] beta;
  real<lower=0> sigma2;
}
transformed parameters {
  vector[N] mu;
  mu = X * beta;
}
model {
  sigma2 ~ inv_gamma(a, b);
  beta ~ normal(0, sqrt(sigma2 / kappa));
  for (i in 1:N) 
    y[i] ~ normal(mu, sqrt(sigma2));
}
generated quantities {
  vector[N_grid] mu_grid;
  mu_grid = X_grid * beta;
}
'
```

```{r}
stan2 <- stan_model(model_code = stan2_code)
```

Create a grid 

```{r}
summary(faithful$waiting)
```

```{r}
N_grid <- 100
m <- min(faithful$waiting)
M <- max(faithful$waiting)
Phi_grid <- data.frame(waiting = seq(m, M, length.out = N_grid)) %>% 
   model.matrix(~ waiting + I(waiting^2 )+ I(waiting^3), data = .)
```


```{r}
standat2 <- list(
  N = nrow(Phi),
  D = ncol(Phi),
  X = Phi,
  y = y,
  a = 1.,
  b = 10,
  kappa = 0.1,
  N_grid = N_grid,
  X_grid = Phi_grid)
```

```{r}
fit2 <- sampling(
  stan2, 
  data = standat2,
  iter = 1000,
  chains = 4,
  pars = c("beta", "sigma2", "mu_grid"),
  cores = 4)
```

This model seems to behabe badly in terms of stability of the parameters (see below). Most likely the behaviour we see is a consequence of the bimodality. I'll combine estimates from chains.

```{r}
rstan::traceplot(fit2, pars = 'beta')
```

[Back to Index](#TOC)

----



### Function space view

Look back at the plot from Exercise 2. We specified a prior distribution over regression parameters, which we can use to obtain a posterior distribution over those regression parameters. But, what we calculated (and plotted) was a posterior distribution over *functions*. Similarly, we can think of our prior on $\beta$ as specifying a prior distribution on the space of cubic functions. Evaluated at a finite number of input locations -- as you did in Exercise 2 -- this posterior distribution is multivariate Gaussian. This is in fact the definition of a Gaussian process: A distribution over functions, such that the marginal distribution evaluated at any finite set of points is multivariate Gaussian.

A priori, the  covariance of $f$ is given by $$\cov(x,x') = E[(f(x)-m(x))(f(x^T)-m(x^T))] = k(x,x')$$. For this reason, our kernel $k$ is often referred to as the covariance function (note, it is a function since we can evaluate it for any pairs $x,x'$). In the above example, where $\beta$ had zero mean, the mean of $f$ is zero; more generally, we will assume some mean function $m(x)$.

Rather than putting a prior distribution over $\beta$, we can specify a covariance function -- remember that our covariance function can be written in terms of the prior covariance of $\beta$.  For example, we might let

$$k(x,x') = \alpha^2\exp\left\{-\frac{1}{2\ell^2}|x-x'|^2\right\}$$
 -- this is known as a squared exponential covariance function, for obvious reasons. This prior encodes the following assumptions:

* The covariance between two datapoints decreases monotonically as the distance between them increases.
* The covariance function is stationary -- it only depends on the distance between $x$ and $x'$, not their locations.
* Even more than being stationary, it is isotropic: It depends only on $|x-x'|$.


### Exercise 3

*Let's explore the resulting distribution over functions. Write some code to sample from a Gaussian process prior with squared exponential covariance function, evaluated on a grid of 200 inputs between 0 and 100. For $\ell=1$, sample 5 functions and plot them on the same plot. Repeat for $\ell=0.1$ and $\ell=10$. Why do we call $\ell$ the lengthscale of the kernel?*




[Back to Index](#TOC)

----

### Exercise 4

*Let $\mathbf{f}_*:=f(\mathbf{X}_*)$ be the function $f$ evaluated at test covariate locations $\mathbf{X}_*$. Derive the posterior distribution $p(\mathbf{f}_*|\mathbf{X}_*,\mathbf{X},\mathbf{y})$, where $\mathbf{y}$ and $\mathbf{X}$ comprise our training set. (You can start from the answer to Exercise 1 if you'd like).*


[Back to Index](#TOC)

----

### Exercise 5

*Return to the faithful dataset. Evaluate the posterior predictive distribution 
$p(\mathbf{f}_*|\mathbf{X}_*,\mathbf{X},\mathbf{y})$, for some reasonable choices of parameters (perhaps explore a few length scales if you're not sure what to pick), and plot the posterior mean plus a 95\% credible interval on a grid of 200 inputs between 0 and 100, overlaying the actual data.*

[Back to Index](#TOC)

----
